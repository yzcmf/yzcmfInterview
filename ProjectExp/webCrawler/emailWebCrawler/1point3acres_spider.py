import asyncio
import aiohttp
import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
import re
import csv
import time

USERNAME = "hu0112174"
PASSWORD = "Zyx!213416"

EMAIL_PATTERN = re.compile(r"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+")
FORUM_BASE = "https://www.1point3acres.com/bbs/"
LIST_BASE = FORUM_BASE + "forum.php?mod=forumdisplay&fid=198&typeid=653&filter=typeid&page="

# âœ… Step 1: ç™»å½•å¹¶è·å– cookies
def get_cookies_after_login(username, password):
    options = Options()
    options.add_argument("--headless")
    driver = webdriver.Chrome(options=options)
    driver.get("https://www.1point3acres.com/bbs/member.php?mod=logging&action=login")
    time.sleep(2)

    driver.find_element(By.NAME, "username").send_keys(username)
    driver.find_element(By.NAME, "password").send_keys(password)
    driver.find_element(By.NAME, "loginsubmit").click()
    time.sleep(5)

    cookies = {c['name']: c['value'] for c in driver.get_cookies()}
    driver.quit()
    return cookies

# âœ… Step 2: è·å–æ‰€æœ‰å¸–å­çš„ URL
def get_thread_urls(pages, cookies):
    session = requests.Session()
    session.cookies.update(cookies)
    urls = []
    for page in range(1, pages + 1):
        print(f"ğŸ“„ æŠ“å–ç¬¬ {page} é¡µ...")
        res = session.get(LIST_BASE + str(page))
        soup = BeautifulSoup(res.text, "html.parser")
        threads = soup.select("a.s.xst")
        for t in threads:
            href = t["href"]
            title = t.get_text(strip=True)
            full_url = FORUM_BASE + href if not href.startswith("http") else href
            urls.append((title, full_url))
    return urls

# âœ… Step 3: å¹¶å‘æŠ“å¸–å­æ­£æ–‡å¹¶æå–é‚®ç®±
async def fetch_emails(session, title, url):
    try:
        async with session.get(url, timeout=15) as res:
            html = await res.text()
            soup = BeautifulSoup(html, "html.parser")
            text = soup.get_text()
            emails = list(set(EMAIL_PATTERN.findall(text)))
            emails = [
                e for e in emails
                if len(e.split('@')[0]) >= 5 and "xx" not in e.lower()
            ]
            if emails:
                print(f"âœ… [{title}] æå– {len(emails)} ä¸ªé‚®ç®±")
            return [title, url, "; ".join(emails)] if emails else None
    except Exception as e:
        print(f"âŒ [{title}] å¤±è´¥ï¼š{e}")
        return None

async def batch_fetch(thread_list, cookies):
    results = []
    async with aiohttp.ClientSession(cookies=cookies) as session:
        tasks = [
            fetch_emails(session, title, url)
            for title, url in thread_list
        ]
        for task in asyncio.as_completed(tasks):
            result = await task
            if result:
                results.append(result)
    return results

# âœ… Step 4: ä¸»å‡½æ•°å…¥å£
def main():
    cookies = get_cookies_after_login(USERNAME, PASSWORD)
    thread_list = get_thread_urls(5, cookies)
    results = asyncio.run(batch_fetch(thread_list, cookies))

    # ä¿å­˜ CSV
    with open("1point3acres_emails_full.csv", "w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow(["Title", "Link", "Emails"])
        writer.writerows(results)
    print("ğŸ‰ å®Œæˆï¼æ•°æ®ä¿å­˜åœ¨ 1point3acres_emails_full.csv")

if __name__ == "__main__":
    main()
